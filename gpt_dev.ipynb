{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m==>\u001b[0m \u001b[1mAuto-updating Homebrew...\u001b[0m\n",
      "Adjust how often this is run with HOMEBREW_AUTO_UPDATE_SECS or disable with\n",
      "HOMEBREW_NO_AUTO_UPDATE. Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).\n",
      "\u001b[34m==>\u001b[0m \u001b[1mAuto-updated Homebrew!\u001b[0m\n",
      "Updated 2 taps (homebrew/services and homebrew/core).\n",
      "\u001b[34m==>\u001b[0m \u001b[1mNew Formulae\u001b[0m\n",
      "haproxy@2.8         openfa              pedump              span-lite\n",
      "\n",
      "You have \u001b[1m5\u001b[0m outdated formulae installed.\n",
      "\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/wget/manifests/1.24.5\u001b[0m\n",
      "######################################################################### 100.0%\n",
      "\u001b[32m==>\u001b[0m \u001b[1mFetching \u001b[32mwget\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/wget/blobs/sha256:9befdad158e59\u001b[0m\n",
      "######################################################################### 100.0%\n",
      "\u001b[34m==>\u001b[0m \u001b[1mPouring wget--1.24.5.arm64_sonoma.bottle.tar.gz\u001b[0m\n",
      "ðŸº  /opt/homebrew/Cellar/wget/1.24.5: 92 files, 4.5MB\n",
      "\u001b[34m==>\u001b[0m \u001b[1mRunning `brew cleanup wget`...\u001b[0m\n",
      "Disable this behaviour by setting HOMEBREW_NO_INSTALL_CLEANUP.\n",
      "Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).\n"
     ]
    }
   ],
   "source": [
    "!brew install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pg52tQwj-Eyk",
    "outputId": "b124dc07-21d7-47e2-b6f6-24c8a437f257"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-06-01 17:04:36--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "WARNING: cannot verify raw.githubusercontent.com's certificate, issued by â€˜CN=DigiCert Global G2 TLS RSA SHA256 2020 CA1,O=DigiCert Inc,C=USâ€™:\n",
      "  Unable to locally verify the issuer's authority.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: â€˜input.txtâ€™\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  5.29MB/s    in 0.2s    \n",
      "\n",
      "2024-06-01 17:04:37 (5.29 MB/s) - â€˜input.txtâ€™ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the tiny Shakespeare dataset\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt --no-check-certificate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "FssIlseR-kXs"
   },
   "outputs": [],
   "source": [
    "# First, we read data from the text file into a Python's variable\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "  text = f.read()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dydu5EKG-u-b",
    "outputId": "828c7645-94e9-49db-bb98-ec5a06fa9d7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WKTyzmZv-2Nn",
    "outputId": "8c08e9be-1c98-45df-bba3-dc36fb3d5939"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# Get the vocabulary out of the data\n",
    "chars = sorted(list(set(text))) # All the unique characters that occur in this text\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qf7kPA8h_w2h"
   },
   "source": [
    "**Vocabulary of the model** is: \"\\\\n!$&\\',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz65\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nU8qdx7AAECJ",
    "outputId": "00707111-150c-4621-ce97-3ab5c988f523"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53, 1, 58, 46, 43, 56, 43, 2]\n",
      "Hello there!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Create tokenizer\"\"\"\n",
    "\n",
    "# Create a mapping from characters to intergers and vice versa\n",
    "stoi = {ch:i for i,ch in enumerate(chars)} # A mapping in Python is in the form of a dictionary.\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Test encoder and decoder\n",
    "print(encode(\"Hello there!\"))\n",
    "print(decode(encode(\"Hello there!\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D366JH5WvkmC"
   },
   "source": [
    "### **What is the trade-off between the length of the sequence of integers - result of the encoder - and the size of the vocabulary?**\n",
    "**There is a trade-off between the length of the sequence of integers - result of the encoder - and the size of the vocabulary.** Encoder having larger-sized vocab usually results in shorter output sequence of integers, and vice versa.\n",
    "Other tokenizer for larger models: OpenAI's tiktoken and Google's sentencepiece, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uw01lk-hxbi_",
    "outputId": "f198bcd7-779f-4408-bc32-d52906173102"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./venv/lib/python3.12/site-packages (2.3.0)\n",
      "Requirement already satisfied: torchvision in ./venv/lib/python3.12/site-packages (0.18.0)\n",
      "Requirement already satisfied: torchaudio in ./venv/lib/python3.12/site-packages (2.3.0)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.12/site-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./venv/lib/python3.12/site-packages (from torch) (4.12.0)\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.12/site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.12/site-packages (from torch) (2024.5.0)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./venv/lib/python3.12/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in ./venv/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QFBrTRcHwZaJ",
    "outputId": "933a1b26-7043-426a-f0de-f35e478115e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# Encode the entire text dataset and store it into a torch.Tensor\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "OO1c7iueyu7K"
   },
   "outputs": [],
   "source": [
    "\"\"\"Create train-test set\"\"\"\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n] # Let 90% of the data to be the train data.\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oe0fS8sL1Xw4"
   },
   "source": [
    "### **How do we feed the data into the Transformer?**\n",
    "We don't ever feed the whole data into the Transformer when it comes to training. Instead, we sample fix-sized chunks of data and feed into the Transformer.\n",
    "Each chunk/block of data contain multiple examples for training. For instance, a block consisting of 8 integers like [18, 47, 56, 57, 58,  1, 15, 47, 58] will be considered as 8 examples: [18], [18,47], [18,47,56], and so on while the Transformer is learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "akSOIjPmzrI0",
    "outputId": "3a661544-6960-43b5-8247-c35528f463e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8 # Maximum context length\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "maqvPDeUzpTx",
    "outputId": "667b23e3-29c9-4ed1-9eec-e84b9085faba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "  context = x[:t+1]\n",
    "  target = y[t]\n",
    "  print(f'when input is {context} the target: {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVeDQ9C221XO"
   },
   "source": [
    "### **What is a batch?**\n",
    "Actually, when we feed chunks of text (actually integers) into the Transformer, we feed a **batch** consisting of multiple chunks of text. So a batch is nothing but a tensor consisting of multiple tensors. This helps utilize the parallel computing power of the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S8_N-uio3x0p",
    "outputId": "bb742bbb-7bf4-492d-d52e-1aa97437f422"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "when input is [24] the target: 43\n",
      "when input is [24, 43] the target: 58\n",
      "when input is [24, 43, 58] the target: 5\n",
      "when input is [24, 43, 58, 5] the target: 57\n",
      "when input is [24, 43, 58, 5, 57] the target: 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
      "when input is [44] the target: 53\n",
      "when input is [44, 53] the target: 56\n",
      "when input is [44, 53, 56] the target: 1\n",
      "when input is [44, 53, 56, 1] the target: 58\n",
      "when input is [44, 53, 56, 1, 58] the target: 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52] the target: 58\n",
      "when input is [52, 58] the target: 1\n",
      "when input is [52, 58, 1] the target: 58\n",
      "when input is [52, 58, 1, 58] the target: 46\n",
      "when input is [52, 58, 1, 58, 46] the target: 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
      "when input is [25] the target: 17\n",
      "when input is [25, 17] the target: 27\n",
      "when input is [25, 17, 27] the target: 10\n",
      "when input is [25, 17, 27, 10] the target: 0\n",
      "when input is [25, 17, 27, 10, 0] the target: 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337) # For reproducability\n",
    "batch_size = 4 # How many independent sequences/chunks/blocks will we process in parallel? = B\n",
    "block_size = 8 # What is the maximum context length for predictions? = T\n",
    "\n",
    "def get_batch(split):\n",
    "  # Generate a small batch of data of inputs x and targets y\n",
    "  data = train_data if split == 'train' else val_data\n",
    "  ix = torch.randint(len(data) - block_size, (batch_size,)) # Generate 4 indices that are smaller than len(data) - block_size\n",
    "                                                            # https://pytorch.org/docs/stable/generated/torch.randint.html\n",
    "  x = torch.stack([data[i:i+block_size] for i in ix]) # Pull out chunks from data and stack 'em up as a 4x8 tensor\n",
    "                                                      # https://pytorch.org/docs/stable/generated/torch.stack.html\n",
    "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "\n",
    "  return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # Batch dimension\n",
    "  for t in range(block_size): # Time dimension\n",
    "    context = xb[b, :t+1]\n",
    "    target = yb[b,t]\n",
    "    print(f'when input is {context.tolist()} the target: {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q-RUIXchkGWl",
    "outputId": "6591fe7b-4e5d-4331-e799-abc791fbc3f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "print(xb) # Our input to the transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lX9QNXahAkkL"
   },
   "source": [
    "### **What is n-gram language model?**\n",
    "**Bigram** or **2-gram language models** are models (a.k.a systems) that predict the next word based on the only preceeding word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h-srXXi1kaZU",
    "outputId": "8b2252a3-efd0-4381-d909-008c3f18a4c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "  # https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
    "\n",
    "  def __init__(self, vocab_size):\n",
    "    super().__init__()\n",
    "    # Each token directly reads off the logits for the next token from a lookup table\n",
    "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # Init the lookup table\n",
    "    # Each of the embedding is a vector of the size 65.\n",
    "\n",
    "  def forward(self, idx, targets=None):\n",
    "\n",
    "    # idx and targets are both (B,T) tensor of integers\n",
    "    logits = self.token_embedding_table(idx)  # (B,T,C) with B = batch_size = 4\n",
    "                                              # T = block_size = 8\n",
    "                                              # C = vocab_size = 65\n",
    "    # Debug\n",
    "    # print(\"logits before\\n\", logits, \"\\n\")\n",
    "    # print(logits.shape)\n",
    "\n",
    "    if targets is None:\n",
    "      loss = None\n",
    "    else:\n",
    "      B, T, C = logits.shape\n",
    "      logits = logits.view(B*T, C)  # B*T is the total number of blocks\n",
    "                                    # in all batches\n",
    "      targets = targets.view(-1)    # or targets = targets.view(B*T)  \n",
    "      loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "    return logits, loss\n",
    "\n",
    "  def generate(self, idx, max_new_tokens):\n",
    "    # idx is (B, T) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "      # get the predictions\n",
    "      logits, loss = self(idx)          # = self.forward(idx)\n",
    "      # focus only on the last time step\n",
    "      logits = logits[:, -1, :]         # For all batches, get their last block\n",
    "                                        # take all elements of those blocks\n",
    "      # apply softmax to get probabilites\n",
    "      probs = F.softmax(logits, dim=-1) # Transform all those blocks above\n",
    "                                        # into probability distribution\n",
    "      # sample from the distribution\n",
    "      idx_next = torch.multinomial(probs, num_samples=1)\n",
    "      # append sampled index to the running sequence\n",
    "      idx = torch.cat((idx, idx_next), dim=1)   # (B, T+1)\n",
    "                                                # idx_next is an index,\n",
    "                                                # which will be concatenated into each and every batch in idx.\n",
    "\n",
    "      # # Debug\n",
    "      # print(\"logits after - the last time step\\n\", logits, \"\\n\")\n",
    "      # print(logits.shape)\n",
    "      # print(\"probs\\n\", probs, \"\\n\")\n",
    "      # print(\"idx_next\\n\", idx_next, \"\\n\")\n",
    "\n",
    "    return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "\n",
    "# print(logits.shape)\n",
    "# print(loss)\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.long) # Input token: a single whitespace\n",
    "\n",
    "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "mUjiowwWL53F"
   },
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ni17eUN-6EJ2",
    "outputId": "4da8e4e6-7f62-42ee-b57a-aa71de995cb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.382369041442871\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(10000):\n",
    "\n",
    "  # sample a batch of data\n",
    "  xb, yb = get_batch('train')\n",
    "\n",
    "  # evaluate the loss\n",
    "  logits, loss = m(xb, yb)\n",
    "  optimizer.zero_grad(set_to_none=True)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "73vVx7a8OdaA",
    "outputId": "c9c591af-f42a-4d25-ce51-768e81bec489"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0, 50, 57, 53,  1, 40, 56,  8,  1, 39, 60, 43,  1, 39, 60, 47, 39, 57,\n",
      "        59, 56, 44,  1, 51, 63,  6,  1, 63, 62, 25, 28, 38, 21,  1, 47, 60, 43,\n",
      "        43,  1, 47, 59, 43, 42, 56, 42,  1, 61, 46, 39, 56,  1, 49, 57, 58, 46,\n",
      "         1, 63,  1, 46,  1, 40, 53, 56, 39,  1, 57,  1, 40, 43,  1, 46, 43, 57,\n",
      "        43,  6,  1, 61, 53, 61, 43, 43, 43, 11,  1, 58, 46, 43,  2,  1, 23, 21,\n",
      "         1,  5, 42, 43,  6,  1, 59, 50, 57, 43, 43, 41, 46, 43, 56, 42,  1, 42,\n",
      "         1, 53,  1, 40, 50, 50, 50, 39, 52, 42, 53, 11, 24, 33, 15, 17, 27,  6,\n",
      "         1, 53, 56, 39, 47, 52, 45, 53, 44, 53, 44,  1, 61, 47, 52,  2,  0, 30,\n",
      "        21, 44, 39, 52, 57,  1, 54, 47, 41, 57, 54, 43, 57, 43, 56, 43, 56,  1,\n",
      "        46, 43, 43,  1, 58, 46, 39,  6,  0, 32, 27, 18, 53, 52, 49, 12,  1, 51,\n",
      "        43,  1, 39, 47, 52,  1, 41, 49, 52, 58, 53, 58, 63,  1, 42, 43, 42,  8,\n",
      "         1, 40, 53,  5, 50, 50, 50, 50,  1, 57, 58,  1, 58, 39,  1, 42, 10,  0,\n",
      "        17, 24, 21, 31,  1, 51, 43,  1, 46, 59, 56, 44,  1, 50, 39, 50,  1, 63,\n",
      "         6,  1, 51, 39,  1, 42, 59, 57,  1, 54, 43,  1, 39, 58, 46, 53, 59, 53,\n",
      "         0, 14, 17, 37, 10,  2,  1, 21, 52, 42, 63, 11,  1, 40, 63,  1, 57,  1,\n",
      "        39, 44, 56, 43, 39, 52, 53, 53,  1, 39, 42, 47, 41, 43, 56, 43, 56, 59,\n",
      "        54, 39,  1, 39, 52, 57, 43,  1, 58, 43, 41, 53, 56, 56, 53,  1, 50, 50,\n",
      "        39, 59, 57,  1, 39,  2,  0, 27, 24, 43, 52, 43, 43, 56, 47, 58, 46, 43,\n",
      "        57, 47, 52, 58, 46, 43, 52, 45, 53, 60, 43,  1, 44, 39, 50,  1, 39, 51,\n",
      "        39, 57,  1, 58, 56, 56,  0, 32, 21,  1, 39, 56,  1, 21,  1, 58,  6,  1,\n",
      "        51, 43, 57,  6,  1, 52,  1, 21, 33, 31, 58,  1, 51, 63,  1, 61,  6,  1,\n",
      "        44, 56, 43, 42, 43, 43, 63, 53, 60, 43,  0, 32, 20, 43, 49,  5,  1, 51,\n",
      "        43, 56, 43, 56,  6,  1, 42, 42,  0, 35, 43,  1, 52, 58, 43, 51,  1, 50,\n",
      "        59, 42,  1, 43, 52, 45, 47, 58, 46, 43, 57, 53, 11,  1, 41, 43, 56,  1,\n",
      "        47, 64, 43,  1, 46, 43, 50, 53, 56, 53, 61, 39, 45, 47, 52, 58, 43,  1,\n",
      "        58, 46, 43, 12,  0, 32, 46, 39, 49,  1, 53, 56, 40, 50, 63, 53, 56, 59,\n",
      "        50, 42, 60, 47, 41, 43, 43,  1, 41, 46, 53, 58,  6,  1, 54,  6,  0, 14,\n",
      "        43, 39, 50, 47, 60, 53, 50, 42, 43,  1, 32, 46,  1, 50, 47])\n",
      "\n",
      "lso br. ave aviasurf my, yxMPZI ivee iuedrd whar ksth y h bora s be hese, woweee; the! KI 'de, ulseecherd d o blllando;LUCEO, oraingofof win!\n",
      "RIfans picspeserer hee tha,\n",
      "TOFonk? me ain ckntoty ded. bo'llll st ta d:\n",
      "ELIS me hurf lal y, ma dus pe athouo\n",
      "BEY:! Indy; by s afreanoo adicererupa anse tecorro llaus a!\n",
      "OLeneerithesinthengove fal amas trr\n",
      "TI ar I t, mes, n IUSt my w, fredeeyove\n",
      "THek' merer, dd\n",
      "We ntem lud engitheso; cer ize helorowaginte the?\n",
      "Thak orblyoruldvicee chot, p,\n",
      "Bealivolde Th li\n"
     ]
    }
   ],
   "source": [
    "result = m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0]\n",
    "print(result)\n",
    "print(decode(result.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MiNs_YbBRPpn"
   },
   "source": [
    "### **The mathematical trick in self-attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vt0NksJcRZFL",
    "outputId": "bb0c13b5-67e3-43ed-ca35-57b574cf874d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Consider the following toy example\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gzGTO0ZzSSLn",
    "outputId": "620beee6-04fa-43f3-8f2f-06249949aa50"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]],\n",
       "\n",
       "        [[ 1.3488, -0.1396],\n",
       "         [ 0.8173,  0.4127],\n",
       "         [-0.1342,  0.4395],\n",
       "         [ 0.2711,  0.4774],\n",
       "         [ 0.2421,  0.0694],\n",
       "         [ 0.0084,  0.0020],\n",
       "         [ 0.0712, -0.1128],\n",
       "         [ 0.2527,  0.2149]],\n",
       "\n",
       "        [[-0.6631, -0.2513],\n",
       "         [ 0.1735, -0.0649],\n",
       "         [ 0.1685,  0.3348],\n",
       "         [-0.1621,  0.1765],\n",
       "         [-0.2312, -0.0436],\n",
       "         [-0.1015, -0.2855],\n",
       "         [-0.2593, -0.1630],\n",
       "         [-0.3015, -0.2293]],\n",
       "\n",
       "        [[ 1.6455, -0.8030],\n",
       "         [ 1.4985, -0.5395],\n",
       "         [ 0.4954,  0.3420],\n",
       "         [ 1.0623, -0.1802],\n",
       "         [ 1.1401, -0.4462],\n",
       "         [ 1.0870, -0.4071],\n",
       "         [ 1.0430, -0.1299],\n",
       "         [ 1.1138, -0.1641]]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "\n",
    "# version 1\n",
    "xbow = torch.zeros((B,T,C)) # \"bow\" stands for \"bag-of-words\".\n",
    "for b in range(B):\n",
    "  for t in range(T):\n",
    "    xprev = x[b,:t+1] # (t,C)\n",
    "    xbow[b,t] = torch.mean(xprev, 0)\n",
    "xbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uBBnwPd1TGig",
    "outputId": "f25145e0-50d3-47e7-d8d6-dac01080c4d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(3, 3)) # Create a lower-triangular matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HoYhKQAmUWq5",
    "outputId": "1fd8c032-ce8a-4086-ae83-b1186fe39508"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / torch.sum(wei, 1, keepdim=True)\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IJzBnkZJVD7D",
    "outputId": "ba092901-b6a1-4120-b312-a5b69453f5e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]],\n",
       "\n",
       "        [[ 1.3488, -0.1396],\n",
       "         [ 0.8173,  0.4127],\n",
       "         [-0.1342,  0.4395],\n",
       "         [ 0.2711,  0.4774],\n",
       "         [ 0.2421,  0.0694],\n",
       "         [ 0.0084,  0.0020],\n",
       "         [ 0.0712, -0.1128],\n",
       "         [ 0.2527,  0.2149]],\n",
       "\n",
       "        [[-0.6631, -0.2513],\n",
       "         [ 0.1735, -0.0649],\n",
       "         [ 0.1685,  0.3348],\n",
       "         [-0.1621,  0.1765],\n",
       "         [-0.2312, -0.0436],\n",
       "         [-0.1015, -0.2855],\n",
       "         [-0.2593, -0.1630],\n",
       "         [-0.3015, -0.2293]],\n",
       "\n",
       "        [[ 1.6455, -0.8030],\n",
       "         [ 1.4985, -0.5395],\n",
       "         [ 0.4954,  0.3420],\n",
       "         [ 1.0623, -0.1802],\n",
       "         [ 1.1401, -0.4462],\n",
       "         [ 1.0870, -0.4071],\n",
       "         [ 1.0430, -0.1299],\n",
       "         [ 1.1138, -0.1641]]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C)\n",
    "xbow2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0PzA2BdrR-s7",
    "outputId": "dfc2056e-0846-4f66-8fc9-915231425290"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(xbow[0], xbow2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EN7gwyOVS8nu",
    "outputId": "909e0cbe-8d7c-4e8a-d06a-2255351bfafa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3: use Softmax\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill_(tril == 0, float(\"-inf\"))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "# xbow3\n",
    "# torch.allclose(xbow, xbow3)   # False?????? But xbow = xbow2\n",
    "torch.allclose(xbow2, xbow3)  # True --> So xbow = xbow3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hBUiMBP6TKP7",
    "outputId": "5769ac14-8f9a-4fa0-c308-b478ee7993d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a =  tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]]) \n",
      "\n",
      "b =  tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]]) \n",
      "\n",
      "c =  tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True) # Normalize the triangular matrix\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "\n",
    "print(\"a = \", a, \"\\n\")\n",
    "print(\"b = \", b, \"\\n\")\n",
    "print(\"c = \", c, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7iJUcqBmPt1"
   },
   "source": [
    "### **Introduce \"self-attention\"**\n",
    "Until now, the 3 version of **BoW** above only calculate the affinities between the current word and words in the past by calculating the average. In other words, we are considering every word in the past has the same relation to the current word. But this is not true in practice. In practice, there are words in the past are far more important to the current word than other ones.\n",
    "In order to solve this problem of calculating \"non-uniform\" affinities, scientists have propose an approach called \"**self-attention**\". So, \"self-attention\" gathers data in the past words, but in a data-independent way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RYqg7sk4mOlD",
    "outputId": "a0e66fd8-a931-4e75-f053-6f517820cef0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32                # batch, time, channels\n",
    "x = torch.randn((B,T,C))\n",
    "\n",
    "# A single self-attention head\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False) # Recall: C = size of a vector embedding\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)                    # (B, T, 16)\n",
    "q = query(x)                  # (B, T, 16)\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ----> (B, T, T)\n",
    "# .tranpose(-2, -1) means we transpose the last two dimensions.\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "# wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill_(tril==0, float(\"-inf\"))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "# out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Crhq9FSD1_b_"
   },
   "source": [
    "### **Example of a wei matrix**\n",
    "\\begin{bmatrix}\n",
    "-1.7629 & -1.3011 & 0.5652 & 2.1616 & -1.0674 & 1.9632 & 1.0765 & -0.4530 \\\\\n",
    "-3.3334 & -1.6556 & 0.1040 & 3.3782 & -2.1825 & 1.0415 & -0.0557 & 0.2927 \\\\\n",
    "-1.0226 & -1.2606 & 0.0762 & -0.3813 & -0.9843 & -1.4303 & 0.0749 & -0.9547 \\\\\n",
    "0.7836 & -0.8014 & -0.3368 & -0.8496 & -0.5602 & -1.1701 & -1.2927 & -1.0260 \\\\\n",
    "-1.2566 & 0.0187 & -0.7880 & -1.3204 & 2.0363 & 0.8638 & 0.3719 & 0.9258 \\\\\n",
    "-0.3126 & 2.4152 & -0.1106 & -0.9931 & 3.3449 & -2.5229 & 1.4187 & 1.2196 \\\\\n",
    "1.0876 & 1.9652 & -0.2621 & -0.3158 & 0.6091 & 1.2616 & -0.5484 & 0.8048 \\\\\n",
    "-1.8044 & -0.4126 & -0.8306 & 0.5899 & -0.7987 & -0.5856 & 0.6433 & 0.6303 \\\\\n",
    "\\end{bmatrix}\n",
    "  \n",
    "**E.g:** In this batch of words, we can see that the 6th token (6th row) in this batch attends to the 5th token the most in its row, at 3.449."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IK2wjJTW4VXc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
